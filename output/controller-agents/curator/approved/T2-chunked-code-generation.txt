# Task: Implement Chunked Code Generation

Priority: P1
Assigned To: Platform Agent
Depends On: None
Estimated Effort: High

## Context
Current limitation: Code generation truncates at ~12,000 tokens for complex projects.
This affects multi-file outputs for larger SaaS applications.
Current workaround is to use 'quality' tier, but this increases costs.

The generation pipeline currently works as:
1. Intent analysis → 2. Architecture planning → 3. Code generation → 4. Context generation

Code generation happens in a single API call, causing truncation for 10+ file projects.

## Objectives
1. Implement file-by-file generation strategy for large outputs
2. Maintain context coherence between file generations
3. Break code generation into chunks based on file count or estimated tokens
4. Stay within existing token budget (optimize, don't increase)

## Success Criteria
- [ ] Complex projects (10+ files) generate completely without truncation
- [ ] No loss of context between file generations
- [ ] Generation cost increase < 20%
- [ ] Test coverage for chunked generation added
- [ ] Streaming UI updated to show per-file progress

## Files to Modify
- packages/ai-agent/src/code-generator.ts (primary - add chunking logic)
- packages/ai-agent/src/index.ts (export new functions if needed)
- website/app/api/generate/project/route.ts (handle chunked responses)
- website/lib/project-generator.ts (update streaming events for chunks)

## Technical Approach
1. Analyze architecture output to estimate file count
2. If files > 10, switch to chunked mode
3. Generate files in batches of 5-7, passing previous files as context
4. Aggregate results before returning final response
5. Add new streaming event type: 'file-progress'

## Handoff
1. Move task to done/
2. Write completion report with before/after metrics (token usage, generation time)
3. Update docs/AI_GENERATION_ENGINE.md with chunked strategy details
4. Notify Testing Agent for validation testing
5. Update memory file
